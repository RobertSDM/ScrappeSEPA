{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "RWon8-CZn9iv",
        "outputId": "e5055102-9c09-489f-94cb-a208936d0b05"
      },
      "outputs": [],
      "source": [
        "!pip install selenium\n",
        "!pip install groq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMvykwjPwdKm"
      },
      "source": [
        "# AI Agent\n",
        "\n",
        "Initializing the agent that will be responsible to extract the authors names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xudT1NWywEXf"
      },
      "outputs": [],
      "source": [
        "from groq import Groq\n",
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JapnM5j8wMIH"
      },
      "outputs": [],
      "source": [
        "model_id = \"llama-3.1-8b-instant\"\n",
        "api_key = userdata.get(\"GROQ_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "SPuI_ShWC7hY"
      },
      "outputs": [],
      "source": [
        "client = Groq(api_key=api_key)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-BLdObjwXIO"
      },
      "source": [
        "# Setting Selenium WebDriver"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "gTQWkxEDQCjR"
      },
      "outputs": [],
      "source": [
        "from selenium.webdriver.common.by import By\n",
        "from selenium import webdriver\n",
        "from bs4 import BeautifulSoup\n",
        "from time import sleep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "IL3JuTe9YO3R"
      },
      "outputs": [],
      "source": [
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument(\"--no-sandbox\")\n",
        "options.add_argument(\"--headless\")\n",
        "options.add_argument(\"--disable-gpu\")\n",
        "options.add_argument(\"--diable-dve-shm-usage\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "collapsed": true,
        "id": "8W71UcypQxQE"
      },
      "outputs": [],
      "source": [
        "wd = webdriver.Chrome(options=options)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "Wb0h2orLDOF0"
      },
      "outputs": [],
      "source": [
        "def run_groq(content: str) -> str:\n",
        "    completition = client.chat.completions.create(\n",
        "    messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": content,\n",
        "            }\n",
        "        ],\n",
        "        model=model_id,\n",
        "    )\n",
        "\n",
        "    return completition.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSAe-FMBHyet"
      },
      "source": [
        "# Page Scrapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "DqjiZ8uLC-gQ"
      },
      "outputs": [],
      "source": [
        "from abc import abstractmethod"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "F6qTib37Iz6l"
      },
      "outputs": [],
      "source": [
        "class SEPAContent():\n",
        "    resource_name: str\n",
        "    discription: str\n",
        "    source: str\n",
        "    resource_type: str\n",
        "    authors: str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "O4wzL3DdFGrD"
      },
      "outputs": [],
      "source": [
        "class PageScrapper():\n",
        "    @abstractmethod\n",
        "    def run(self, links: list[str]) -> list[str]:\n",
        "        raise NotImplementedError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "2n-PxakoDIEf"
      },
      "outputs": [],
      "source": [
        "class SEPACaseStudiePageScrapper(PageScrapper):\n",
        "    def run(self, links: list[str]) -> list[SEPAContent]:\n",
        "        contents = []\n",
        "        for link in links:\n",
        "            print(f\"SCRAPPING {link}\")\n",
        "            content = SEPAContent()\n",
        "\n",
        "            wd.get(link)\n",
        "            soup = BeautifulSoup(wd.page_source, \"html.parser\")\n",
        "\n",
        "            content.resource_name = soup.find(\"meta\", {\"property\": \"og:title\"})[\"content\"].replace(\"| SEPA\", \"\").strip()\n",
        "            content.discription = soup.find(\"meta\", {\"name\": \"description\"})[\"content\"]\n",
        "            content.resource_type = soup.find(\"meta\", {\"property\": \"og:type\"})[\"content\"]\n",
        "            content.source = link\n",
        "\n",
        "            contents.append(content.__dict__)\n",
        "\n",
        "            sleep(10)\n",
        "\n",
        "        return contents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "69mhYvC4DgJb"
      },
      "outputs": [],
      "source": [
        "class SEPAReportPageScrapper(PageScrapper):\n",
        "    def run(self, links: list[str]) -> list[SEPAContent]:\n",
        "        contents = []\n",
        "        for link in links:\n",
        "            print(f\"SCRAPPING {link}\")\n",
        "            content = SEPAContent()\n",
        "\n",
        "            wd.get(link)\n",
        "            soup = BeautifulSoup(wd.page_source, \"html.parser\")\n",
        "\n",
        "            try:\n",
        "                title_type = soup.find(\"div\", {\"class\": \"column v-align-middle column-2\"}).find(\"div\", {\"class\": \"content\"})\n",
        "                content.resource_type = title_type.find(\"span\").text\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "            content.resource_name = soup.title.text.replace(\"| SEPA\", \"\").strip()\n",
        "            content.discription = soup.find(\"meta\", {\"name\": \"description\"})[\"content\"]\n",
        "            content.source = link\n",
        "\n",
        "            authors_container = soup.find(\"div\", {\"class\": \"entry\"})\n",
        "            cleaned_authors = \"\"\n",
        "            for c in authors_container:\n",
        "                paragraph = c.get_text() + \"\\n\"\n",
        "                cleaned_authors += paragraph\n",
        "\n",
        "            authors = run_groq(f\"\"\"\n",
        "Extract for me only the authors names, they are humans not companies, from the text. Put the names in a string separated by a ', '.\n",
        "If none author is found return an empty string. Return only the asked task:\n",
        "{cleaned_authors}\n",
        "\"\"\")\n",
        "\n",
        "            content.authors = authors\n",
        "            contents.append(content.__dict__)\n",
        "\n",
        "            sleep(10)\n",
        "\n",
        "        return contents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "lzzmFexcDsNk"
      },
      "outputs": [],
      "source": [
        "class SEPAWhitePaperPageScrapper(PageScrapper):\n",
        "    def run(self, links: list[str]) -> list[SEPAContent]:\n",
        "        contents = []\n",
        "        for link in links:\n",
        "            print(f\"SCRAPPING {link}\")\n",
        "            content = SEPAContent()\n",
        "\n",
        "            wd.get(link)\n",
        "            soup = BeautifulSoup(wd.page_source, \"html.parser\")\n",
        "\n",
        "            try:\n",
        "                title_type = soup.find(\"div\", {\"class\": \"column v-align-middle column-2\"}).find(\"div\", {\"class\": \"content\"})\n",
        "                content.resource_type = title_type.find(\"span\").text\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "            content.resource_name = soup.title.text.replace(\"| SEPA\", \"\").strip()\n",
        "            content.discription = soup.find(\"meta\", {\"name\": \"description\"})[\"content\"]\n",
        "            content.source = link\n",
        "\n",
        "\n",
        "            authors_container = soup.find(\"div\", {\"class\": \"entry\"})\n",
        "            cleaned_authors = \"\"\n",
        "            for c in authors_container:\n",
        "                paragraph = c.get_text() + \"\\n\"\n",
        "                cleaned_authors += paragraph\n",
        "\n",
        "            authors = run_groq(f\"\"\"\n",
        "Extract for me only the authors names, they are humans not companies, from the text. Put the names in a string separated by a ', '.\n",
        "If none author is found return an empty string. Return only the asked task:\n",
        "{cleaned_authors}\n",
        "\"\"\")\n",
        "            content.authors = authors\n",
        "            contents.append(content.__dict__)\n",
        "\n",
        "            sleep(10)\n",
        "\n",
        "        return contents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCeLtzaJH3JV"
      },
      "source": [
        "# Getting the Content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "WPd0iihMBox1"
      },
      "outputs": [],
      "source": [
        "def get_links_to_scrappe(url: str, limit: int | None = None) -> list[str]:\n",
        "    wd.get(url)\n",
        "\n",
        "    try:\n",
        "        last_page = wd.find_element(By.CSS_SELECTOR, \"a[class='facetwp-page last']\")\n",
        "        last_page = int(last_page.text)\n",
        "    except:\n",
        "        last_page = 1\n",
        "\n",
        "    page_links = []\n",
        "    for page in range(0, last_page):\n",
        "        wd.get(url + f\"&_paged={page + 1}\")\n",
        "\n",
        "        html = wd.page_source\n",
        "        soup = BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "        links = soup.find_all(\"a\", {\"class\": \"button button-small cta\"})\n",
        "        for link in links:\n",
        "            page_links.append(link[\"href\"])\n",
        "\n",
        "        sleep(4)\n",
        "\n",
        "    print(f\"\\nIn the page {url}, {len(page_links) if not limit else limit} links were found\")\n",
        "\n",
        "    if limit and len(page_links) >= limit:\n",
        "        return page_links[:limit]\n",
        "    else:\n",
        "        return page_links"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "X_C6abOPAJNg"
      },
      "outputs": [],
      "source": [
        "class PageToScrappe():\n",
        "    def __init__(self, page_scrapper: PageScrapper, url: str):\n",
        "        self.url = url\n",
        "        self.page_scrapper = page_scrapper\n",
        "\n",
        "    def run(self, links: list[str]) -> list[str]:\n",
        "        return self.page_scrapper.run(links)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "ITLgOEgMADuQ"
      },
      "outputs": [],
      "source": [
        "pages_to_scrappe = [\n",
        "    PageToScrappe(SEPACaseStudiePageScrapper(), \"https://sepapower.org/knowledge/?_type=case-study&_publication_period=last-5-years\"),\n",
        "    PageToScrappe(SEPAReportPageScrapper(), \"https://sepapower.org/knowledge/?_type=report&_publication_period=last-5-years\"),\n",
        "    PageToScrappe(SEPAWhitePaperPageScrapper(), \"https://sepapower.org/knowledge/?_type=white-paper&_publication_period=last-5-years\"),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "GbinHCe_F6sZ"
      },
      "outputs": [],
      "source": [
        "scrapped_content = []\n",
        "LIMIT = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "vcsPmxTZCJww",
        "outputId": "9d38aaff-b9d9-475c-c193-e08a2d375eb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In the page https://sepapower.org/knowledge/?_type=report&_publication_period=last-5-years ,  3 links were found\n",
            "SCRAPPING https://sepapower.org/resource/application-guide-sepa-interoperability-profile-for-electric-vehicle-fleet-managed-charging-utilizing-ieee-2030-5-2018/\n",
            "SCRAPPING https://sepapower.org/resource/50-states-of-virtual-power-plant-and-supporting-distributed-energy-resources-2024-state-policy-snapshot/\n",
            "SCRAPPING https://sepapower.org/resource/advancing-building-electrification-utility-case-studies/\n",
            "In the page https://sepapower.org/knowledge/?_type=white-paper&_publication_period=last-5-years ,  3 links were found\n",
            "SCRAPPING https://sepapower.org/resource/decoding-derms-options-for-the-future-of-der-management/\n",
            "SCRAPPING https://sepapower.org/resource/conformity-assessment-for-smart-grid-electromagnetic-compatibility-emc/\n",
            "SCRAPPING https://sepapower.org/resource/benchmarking-equitable-transportation-electrification/\n"
          ]
        }
      ],
      "source": [
        "for page in pages_to_scrappe:\n",
        "    links = get_links_to_scrappe(page.url, LIMIT)\n",
        "    contents = page.run(links)\n",
        "    for content in contents:\n",
        "        scrapped_content.append(content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "WgnxhHlFhaCV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "DImwA8B-ifC1"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame.from_dict(scrapped_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "KyAC2XyCQhhu"
      },
      "outputs": [],
      "source": [
        "df.to_csv(\"SEPA_data.csv\", index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "QMvykwjPwdKm",
        "o-BLdObjwXIO",
        "kSAe-FMBHyet"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
