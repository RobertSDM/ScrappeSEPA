{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "RWon8-CZn9iv",
        "outputId": "bf05df28-bb12-43af-ff6f-75ee945fe4a6"
      },
      "outputs": [],
      "source": [
        "!pip install selenium\n",
        "!pip install groq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMvykwjPwdKm"
      },
      "source": [
        "# LLM\n",
        "\n",
        "Initializing the LLM that will be responsible to extract the authors names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {
        "id": "xudT1NWywEXf"
      },
      "outputs": [],
      "source": [
        "from groq import Groq\n",
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {
        "id": "JapnM5j8wMIH"
      },
      "outputs": [],
      "source": [
        "model_id = \"llama-3.1-8b-instant\"\n",
        "api_key = userdata.get(\"GROQ_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {
        "id": "SPuI_ShWC7hY"
      },
      "outputs": [],
      "source": [
        "client = Groq(api_key=api_key)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-BLdObjwXIO"
      },
      "source": [
        "# Setting Selenium WebDriver"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {
        "id": "gTQWkxEDQCjR"
      },
      "outputs": [],
      "source": [
        "from selenium.webdriver.common.by import By\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.webdriver import WebDriver\n",
        "from bs4 import BeautifulSoup\n",
        "from time import sleep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {
        "id": "IL3JuTe9YO3R"
      },
      "outputs": [],
      "source": [
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument(\"--no-sandbox\")\n",
        "options.add_argument(\"--headless\")\n",
        "options.add_argument(\"--disable-gpu\")\n",
        "options.add_argument(\"--diable-dve-shm-usage\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {
        "id": "_U3pxNIk69vV"
      },
      "outputs": [],
      "source": [
        "def webdriver_factory():\n",
        "    while True:\n",
        "        try:\n",
        "            wd = webdriver.Chrome(options=options)\n",
        "            yield wd\n",
        "        finally:\n",
        "            wd.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {
        "id": "Wb0h2orLDOF0"
      },
      "outputs": [],
      "source": [
        "def run_groq(content: str) -> str:\n",
        "    prompt = (f\"\"\"\n",
        "Extract for me only the authors names.\n",
        "\n",
        "IMPORTANT:\n",
        "- You must extract only the human names not anything else from the text.\n",
        "- The returned content should be a string with the names separated by a ', '.\n",
        "- If no name is found return 'None'.\n",
        "- Pay attention to return only what was asked, nothing more.\n",
        "- Just stick to the content.\n",
        "\n",
        "TEXT:\n",
        "\n",
        "{content}\n",
        "\"\"\")\n",
        "\n",
        "    completition = client.chat.completions.create(\n",
        "    messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt,\n",
        "            }\n",
        "        ],\n",
        "        model=model_id,\n",
        "    )\n",
        "\n",
        "    return completition.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSAe-FMBHyet"
      },
      "source": [
        "# Page Scrapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {
        "id": "DqjiZ8uLC-gQ"
      },
      "outputs": [],
      "source": [
        "from abc import abstractmethod"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "metadata": {
        "id": "F6qTib37Iz6l"
      },
      "outputs": [],
      "source": [
        "class SEPAContent():\n",
        "    resource_name: str\n",
        "    discription: str\n",
        "    source: str\n",
        "    resource_type: str\n",
        "    authors: str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {
        "id": "O4wzL3DdFGrD"
      },
      "outputs": [],
      "source": [
        "class PageScrapper():\n",
        "    @abstractmethod\n",
        "    def run(self, links: list[str], wd: WebDriver, waitTime: int = 5) -> list[str]:\n",
        "        raise NotImplementedError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {
        "id": "2n-PxakoDIEf"
      },
      "outputs": [],
      "source": [
        "class SEPACaseStudiePageScrapper(PageScrapper):\n",
        "    def run(self, links: list[str], wd: WebDriver, waitTime: int = 5) -> list[SEPAContent]:\n",
        "        contents = []\n",
        "        for link in links:\n",
        "            print(f\"SCRAPPING {link}\")\n",
        "            content = SEPAContent()\n",
        "\n",
        "            wd.get(link)\n",
        "            soup = BeautifulSoup(wd.page_source, \"html.parser\")\n",
        "\n",
        "            content.resource_name = soup.find(\"meta\", {\"property\": \"og:title\"})[\"content\"].replace(\"| SEPA\", \"\").strip()\n",
        "            content.discription = soup.find(\"meta\", {\"name\": \"description\"})[\"content\"]\n",
        "            content.resource_type = soup.find(\"meta\", {\"property\": \"og:type\"})[\"content\"]\n",
        "            content.source = link\n",
        "\n",
        "            contents.append(content.__dict__)\n",
        "\n",
        "            sleep(waitTime)\n",
        "\n",
        "        return contents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {
        "id": "69mhYvC4DgJb"
      },
      "outputs": [],
      "source": [
        "class SEPAReportPageScrapper(PageScrapper):\n",
        "    def run(self, links: list[str], wd: WebDriver, waitTime: int = 5) -> list[SEPAContent]:\n",
        "        contents = []\n",
        "        for link in links:\n",
        "            print(f\"SCRAPPING {link}\")\n",
        "            content = SEPAContent()\n",
        "\n",
        "            wd.get(link)\n",
        "            soup = BeautifulSoup(wd.page_source, \"html.parser\")\n",
        "\n",
        "            try:\n",
        "                title_type = soup.find(\"div\", {\"class\": \"column v-align-middle column-2\"}).find(\"div\", {\"class\": \"content\"})\n",
        "                content.resource_type = title_type.find(\"span\").text\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "            content.resource_name = soup.title.text.replace(\"| SEPA\", \"\").strip()\n",
        "            content.discription = soup.find(\"meta\", {\"name\": \"description\"})[\"content\"]\n",
        "            content.source = link\n",
        "\n",
        "            authors_container = soup.find(\"div\", {\"class\": \"entry\"})\n",
        "            cleaned_authors = \"\"\n",
        "            for c in authors_container:\n",
        "                paragraph = c.get_text() + \"\\n\"\n",
        "                cleaned_authors += paragraph\n",
        "\n",
        "            authors = run_groq({cleaned_authors})\n",
        "\n",
        "            content.authors = authors\n",
        "            contents.append(content.__dict__)\n",
        "\n",
        "            sleep(waitTime)\n",
        "\n",
        "        return contents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {
        "id": "lzzmFexcDsNk"
      },
      "outputs": [],
      "source": [
        "class SEPAWhitePaperPageScrapper(PageScrapper):\n",
        "    def run(self, links: list[str], wd: WebDriver, waitTime: int = 5) -> list[SEPAContent]:\n",
        "        contents = []\n",
        "        for link in links:\n",
        "            print(f\"SCRAPPING {link}\")\n",
        "            content = SEPAContent()\n",
        "\n",
        "            wd.get(link)\n",
        "            soup = BeautifulSoup(wd.page_source, \"html.parser\")\n",
        "\n",
        "            try:\n",
        "                title_type = soup.find(\"div\", {\"class\": \"column v-align-middle column-2\"}).find(\"div\", {\"class\": \"content\"})\n",
        "                content.resource_type = title_type.find(\"span\").text\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "            content.resource_name = soup.title.text.replace(\"| SEPA\", \"\").strip()\n",
        "            content.discription = soup.find(\"meta\", {\"name\": \"description\"})[\"content\"]\n",
        "            content.source = link\n",
        "\n",
        "\n",
        "            authors_container = soup.find(\"div\", {\"class\": \"entry\"})\n",
        "            cleaned_authors = \"\"\n",
        "            for c in authors_container:\n",
        "                paragraph = c.get_text() + \"\\n\"\n",
        "                cleaned_authors += paragraph\n",
        "\n",
        "            authors = run_groq({cleaned_authors})\n",
        "\n",
        "            content.authors = authors\n",
        "            contents.append(content.__dict__)\n",
        "\n",
        "            sleep(waitTime)\n",
        "\n",
        "        return contents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCeLtzaJH3JV"
      },
      "source": [
        "# Getting the Content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {
        "id": "WPd0iihMBox1"
      },
      "outputs": [],
      "source": [
        "def get_links_to_scrappe(url: str, wd: WebDriver, limit: int | None = None) -> list[str]:\n",
        "    wd.get(url)\n",
        "\n",
        "    try:\n",
        "        last_page = wd.find_element(By.CSS_SELECTOR, \"a[class='facetwp-page last']\")\n",
        "        last_page = int(last_page.text)\n",
        "    except:\n",
        "        last_page = 1\n",
        "\n",
        "    page_links = []\n",
        "    for page in range(0, last_page):\n",
        "        wd.get(url + f\"&_paged={page + 1}\")\n",
        "\n",
        "        html = wd.page_source\n",
        "        soup = BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "        links = soup.find_all(\"a\", {\"class\": \"button button-small cta\"})\n",
        "        for link in links:\n",
        "            page_links.append(link[\"href\"])\n",
        "\n",
        "        sleep(4)\n",
        "\n",
        "    print(f\"\\nIn the page {url}, {len(page_links) if not limit else limit} links were found\")\n",
        "\n",
        "    if limit and len(page_links) >= limit:\n",
        "        return page_links[:limit]\n",
        "    else:\n",
        "        return page_links"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {
        "id": "X_C6abOPAJNg"
      },
      "outputs": [],
      "source": [
        "class PageToScrappe():\n",
        "    def __init__(self, page_scrapper: PageScrapper, url: str):\n",
        "        self.url = url\n",
        "        self.page_scrapper = page_scrapper\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "metadata": {
        "id": "ITLgOEgMADuQ"
      },
      "outputs": [],
      "source": [
        "pages_to_scrappe = [\n",
        "    PageToScrappe(SEPACaseStudiePageScrapper(), \"https://sepapower.org/knowledge/?_type=case-study&_publication_period=last-5-years\"),\n",
        "    PageToScrappe(SEPAReportPageScrapper(), \"https://sepapower.org/knowledge/?_type=report&_publication_period=last-5-years\"),\n",
        "    PageToScrappe(SEPAWhitePaperPageScrapper(), \"https://sepapower.org/knowledge/?_type=white-paper&_publication_period=last-5-years\"),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "metadata": {
        "id": "GbinHCe_F6sZ"
      },
      "outputs": [],
      "source": [
        "scrapped_content = []\n",
        "LIMIT = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {
        "id": "N2OuhM429nJD"
      },
      "outputs": [],
      "source": [
        "wd = webdriver_factory()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "vcsPmxTZCJww",
        "outputId": "baddd256-d629-4946-a303-f4ba3098ebba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "In the page https://sepapower.org/knowledge/?_type=case-study&_publication_period=last-5-years, 3 links were found\n",
            "SCRAPPING https://sepapower.org/resource/case-study-hurricane-helene-hot-springs-microgrid/\n",
            "SCRAPPING https://sepapower.org/resource/west-virginia-regional-microgrids-for-resilience-study/\n",
            "SCRAPPING https://sepapower.org/resource/case-studies-for-accelerating-coordinated-utility-program-for-grid-interactive-efficient-buildings/\n",
            "\n",
            "In the page https://sepapower.org/knowledge/?_type=report&_publication_period=last-5-years, 3 links were found\n",
            "SCRAPPING https://sepapower.org/resource/inclusive-utility-investment-guide-for-distributed-energy-resources/\n",
            "SCRAPPING https://sepapower.org/resource/application-guide-sepa-interoperability-profile-for-electric-vehicle-fleet-managed-charging-utilizing-ieee-2030-5-2018/\n",
            "SCRAPPING https://sepapower.org/resource/50-states-of-virtual-power-plant-and-supporting-distributed-energy-resources-2024-state-policy-snapshot/\n",
            "\n",
            "In the page https://sepapower.org/knowledge/?_type=white-paper&_publication_period=last-5-years, 3 links were found\n",
            "SCRAPPING https://sepapower.org/resource/decoding-derms-options-for-the-future-of-der-management/\n",
            "SCRAPPING https://sepapower.org/resource/conformity-assessment-for-smart-grid-electromagnetic-compatibility-emc/\n",
            "SCRAPPING https://sepapower.org/resource/benchmarking-equitable-transportation-electrification/\n"
          ]
        }
      ],
      "source": [
        "for page in pages_to_scrappe:\n",
        "    driver = next(wd)\n",
        "    links = get_links_to_scrappe(page.url, driver, LIMIT)\n",
        "    contents = page.page_scrapper.run(links, driver)\n",
        "    for content in contents:\n",
        "        scrapped_content.append(content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "metadata": {
        "id": "WgnxhHlFhaCV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {
        "id": "DImwA8B-ifC1"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame.from_dict(scrapped_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {
        "id": "KyAC2XyCQhhu"
      },
      "outputs": [],
      "source": [
        "df.to_csv(\"SEPA_data.csv\", index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
